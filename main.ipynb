{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5EjUM2EKoVL"
   },
   "source": [
    "# On Suicidality Subject Matter Detection\n",
    "\n",
    "The purpose of this study is to create a predictive model that assesses text and detects in its subject matter content that relates to suicidality, and to examine the ways in which the results can facilitate analysis and prevention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWEnySMMLQ_n"
   },
   "source": [
    "### Table of Contents:\n",
    "\n",
    "- Part I: Introduction\n",
    "- Part II: Preprocessing Summary\n",
    "- Part III: EDA Summary\n",
    "- Part IV: Predictive Model and Relevant Findings\n",
    "- Part V: Streamlit Application\n",
    "- Part VI: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUdu8YWhLs41"
   },
   "source": [
    "## Part I: Introduction\n",
    "\n",
    "While sufferers of depression are not and must not be assumed inherently suicidal, it can often serve as a contributing cause in suicidality. Thus, questions arise: Can reliable means be developed to detect distinctions between subject matter authored by sufferers of depression or that remains within the threshold of depression and subject matter that crosses a threshold into suicidality, or at least whose focus is suicidality? What would such means looks like, and to what uses might we use them? Most importantly, how might the development and improvement of such detection aid in future suicide prevention?\n",
    "\n",
    "This study aims primarily to lay a foundation for answering these questions by developing a functional predictive model that can distinguish reliably between text bodies on either side of this threshold (i.e. depression and suicidality) and to allow the findings and corrollary Streamlit application to act as a stepping stone for further inquiry.\n",
    "\n",
    "It must be stated that nothing within this study ought to be taken as the equivalent of or a substitute for real licensed medical treatment. \n",
    "\n",
    "During the course of this project, the data (consisting of text posts collected by Nikhileswar Komati from `r/SuicideWatch` and `r/depression` using the PushShift API) was fit to many binary classification models, in a variety of forms, or having undergone a variety of different \"versions\" of preprocessing. Though the initial plan was to employ two final models (one for interpretability and on for sheer predictive power), in the end the model that proved the most accurate also lends itself more than the others to actionable interpretation. More on this model can be found below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcnU6wVFReE2"
   },
   "source": [
    "## Part II: Preprocessing Summary\n",
    "\n",
    "The notebooks in which preprocessing work was done for this project can be found in `appendices/preprocessing/`, but a brief summary will be offered here. The initial dataset used is a collection of over 700,000 combined entries from the `r/SuicideWatch` and `r/depression` subreddits, gathered by user Nikhileswar Komati using the PushShift API.\n",
    "\n",
    "First, relevant text data was combined and parsed, with null entries being removed due to the large size of the dataset. The target column (`Subreddit`) was converted to a binary integer column for modeling purposes, and after parsing the text of problematic characters, it was converted separately to two separate feature columns (`lower`, or the combined text made lower case, and `unstopped`, identical to the `lower` column but with English stopwords removed).\n",
    "\n",
    "These steps are recorded in `01_initial_cleaning.ipynb` of the forementioned directory and subdirectory.\n",
    "\n",
    "What followed was a great effort to create multiple forms of each text, with each form serving as a potential feature, so that all versions could see the modeling phase and the one that offered the best accuracy could be used in the final model. Ultimately, eight different versions of the text data were created:\n",
    "\n",
    "- words only, made lowercase, lemmatized\n",
    "- words only, made lowercase, stemmed\n",
    "- words made lowercase and potentially significant digits, lemmatized\n",
    "- words made lowercase and potentially significant digits, stemmed\n",
    "- words only with English stopwords removed, made lowercase, lemmatized\n",
    "- words only with English stopwords removed, made lowercase, stemmed\n",
    "- words made lowercase (with English stopwords removed) and potentially significant digits, lemmatized\n",
    "- words made lowercase (with English stopwords removed) and potentially significant digits, stemmed\n",
    "\n",
    "The generation of these features is again recorded in `appendices/preprocessing/`, in the following notebooks:\n",
    "\n",
    "- `02_cleaning_lower_lemm.ipynb`\n",
    "- `03_cleaning_unstopped_lemm.ipynb`\n",
    "- `04_cleaning_lower_stem.ipynb`\n",
    "- `05_cleaning_unstopped_stem.ipynb`\n",
    "\n",
    "The notebook that records exploratory analysis for this project is `06_initial_eda.ipynb`, and of the eight notebooks in. the `preprocessing` subdirectory, it is the one that stands out as having markdown included for explanation. (The other notebooks are largely self-explanatory, with very simple code actions taken, and what little explanation is necessary is offered here).\n",
    "\n",
    "As the exploratory phase is best treated separately from initial cleaning, more notes on this notebook are included below.\n",
    "\n",
    "With the eight features (or eight versions of the same feature) created, it became most reasonable to combine them into a single dataset, and this was done in `07_full_dataset.ipynb`. The file that gets exported in the last cell is the most robust and comprehensive version of the relevant data for this project. However, because of its sheer size, computational limits and RAM limits made it necessary in the early stages of modeling to down sample significantly, and the significantly smaller (though still large) down-sampled dataset was created in `08_down_sample.ipynb`. This ultimately became the \"official\" dataset of the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYj2-GfdXKtN"
   },
   "source": [
    "## Part III: EDA\n",
    "\n",
    "The exploratory phase of this study was done for the most part in conjunction with the projects first check-in deadline, and for that reason, the relevant notebook (`06_initial_eda.ipynb`) is marked down in great deal with what was discovered. Very briefly though, the most notable finding was the significantly shorter length of posts to `r/SuicideWatch` on average than to `r/depression`. Both in terms of word count and character count, posts to `r/depression` were roughly 10 to 20% longer than posts to `r/SuicideWatch`. What this might mean could invite further inquiry as part of a future project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNvIa_R8YV8_"
   },
   "source": [
    "## Part IV: Predictive Model and Relevant Findings\n",
    "\n",
    "Many versions of the data were fit to many models. The directory `appendices/modeling` contains over twenty notebooks whose shared purpose was simply to find the best model for the best version of the available data to obtain the most reliable results possible.\n",
    "\n",
    "All eight versions of the preprocess text data were fit. Models included logistic regression, Gradient Boosting, AdaBoost, random forests, multinomial naive Bayes, Bernoulli naive Bayes, and neural networks. The running of these models, their scores and outputs can be found in notebooks `01` through `13` of the `modeling` subdirectory. \n",
    "\n",
    "Toward the end of the modeling phase, high training scores for random forests and neural networks led to more extensive attempts at fine-tuning, to see if this or that combination of hyperparameters might give the test data a significant jump. (See `14` through `19` in `modeling`.) Ultimately though, logistic regression outperformed these and all the others, making it the strongest model of all both for predictive capacity and interpretability. \n",
    "\n",
    "Due to RAM limitations, gridsearching multiple vectorized dataframes to fine-tune the final logistic regression model was not feasible. This led to testing hyperparameter combinations for vectorizing the text one at a time and finally settling on the best result. (See `20` in `modeling`.) Finally, the final polishing of the logistic regression parameters is found in `21` of `modeling`, and in the end, the default variables gave the best score for the test data.\n",
    "\n",
    "Initial imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ini6jVq_Ka3G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmDm_UyObZxB"
   },
   "source": [
    "Mounting drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlHY4Lh3bYlD",
    "outputId": "df80e4f3-51ae-4c45-d9ef-827ed0b6025b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbpuGDMobjH6"
   },
   "source": [
    "Reading in dataset. The best-performing version of the text data proved to be words-only, lemmatized, with English stopwords removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zoryEiWZbcDV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/down_sample.csv')[['Subreddit', 'unst_simp_lem']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYZJ7NRub1qf"
   },
   "source": [
    "Divvying input and output columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iI0cmVmEbwdc"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns='Subreddit')\n",
    "y = df['Subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcMwxBWYcDVd"
   },
   "source": [
    "`TfidfVectorizer` proved the most accurate. Vectorization below, with best hyperparameters included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iHoDbjIub5ao"
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(min_df=5, max_df=0.80, ngram_range=(1,3))\n",
    "tvec.fit(X_train['unst_simp_lem'])\n",
    "X_train_tv = pd.DataFrame(tvec.transform(X_train['unst_simp_lem']).todense(), columns = tvec.get_feature_names())\n",
    "X_test_tv = pd.DataFrame(tvec.transform(X_test['unst_simp_lem']).todense(), columns = tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7PZv6t6cedL"
   },
   "source": [
    "Model instantiated and fit in the cells that follow. Again, as it turned out, the default hyperparameter settings offered the highest test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8Zj1fQm2cL8A"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzhYeaXpcdeB",
    "outputId": "a4d14a87-1981-4d6e-dd31-a0064a69dc60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_tv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gq_UFDfYcxmq",
    "outputId": "6720c7d0-4dea-4bb5-bcc6-150bbccacd86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8848444444444444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train_tv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiCiLHdYcxwR",
    "outputId": "2df3efd6-ebdb-43ac-990d-40047e6847f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7974666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_tv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZbAPSZVc8Fg",
    "outputId": "432e28fb-8cbd-46fd-bcf0-fd2d004232ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7886222222222223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, X_train_tv, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJii-UoEefq0"
   },
   "source": [
    "The score nears the 80% cusp for accuracy, against a 50% baseline (the subreddits are represented evenly both in the initial and final dataset). \n",
    "\n",
    "Because of the nature of the question this study seeks to answer, recall scores are arguably more significant than accuracy scores, or they could be potentially for any scenario in which the goal is detection of suicidality. That is, avoiding false negatives, or missing cases of suicidality, could be paramount. (Again, this study does not claim to have finalized a model that can be put to this purpose, nor is it a proxy for actual diagnoses.)\n",
    "\n",
    "Throughout the modeling process, recall scores tended to correlate generally with their respective accuracy scores. Models with better accuracy had better recall scores and vice versa.\n",
    "\n",
    "It is beyond the scope of this study to determine the \"ideal\" balance between accuracy and recall. A question of that magnitude requires great care and only the most judicious methods of inquiry. However, the Streamlit application (discussed in the section below), was developed with the fine-tuning of these metrics in mind. For this section, a brief confusion matrix is included to give a sense of how recall looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "VCrUU16fgOIV",
    "outputId": "80192417-e3cc-413b-8e0f-ccc9d173d30e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, lr.predict(X_test_tv))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=lr.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3rE-Gfngv1n"
   },
   "source": [
    "For a more thorough treatment of takeaways that this model provides, see `presentation.pdf`, included in this repo. However, the comparative values of model coefficients for the vectorized data offer some insights. The most significant terms that might lead to a positive classification (or, a classification of containing subject matter related to suicidality) turn out to be terms that seem directly related to dying, isolation, and feelings of worthlessness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAz_gaI-hn41"
   },
   "source": [
    "## Part V: Streamlit Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W57fdc-Nhpou"
   },
   "source": [
    "The Streamlit application at the time of this writing is extremely rudimentary, but also built according to a design that invites future expansion. Its main function is to take in a body of text from the user as well as a specified probability threshold.\n",
    "\n",
    "This threshold allows the user to optimize according to any predetermined probability of his or her choosing that the text being submitted contains subject matter related to suicidality, according to the model. Again, the purpose is to allow the user to adjust for optimal recall (so to speak), based on the limit deemed most appropriate for the use case in question.\n",
    "\n",
    "The model is called neatly, and is coded in such a way that should a future model perform better, it can be easily swapped in.\n",
    "\n",
    "An `About` page is included, so that if the project ever expands, more authors can be added, along with their contact information so that users can consult them according to their respective areas of expertise. Finally, a `Resources` page is included, to which more items can be added, for anyone seeking more information on suicide prevention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca0aNh50jB2b"
   },
   "source": [
    "## Part VI: Conclusion\n",
    "\n",
    "Finally, the model works. Its implications are again somewhat unclear and would require further inquiry to iron out. Suicidality as subject matter can be said to be reliably detectable, but to what degree does suicidality as subject matter correlate with suicidality as such? This is a fair question, but a cursory perusal of the data suggest that the correlation in many cases is indeed present. More data from additional sources could fit to a model to make it more well-rounded, helping to bolster its predictive power in relation to a somewhat nebulous concept. \n",
    "\n",
    "Could further developments connect specific terms with higher coefficients to known warning signs of suicidality, so that users might receive an output of what to read for in their input? This might be possible, but it also enters the realm of \"diagnosis\", and therefore must be treated with the greatest caution. Could known cases of suicide provide data that might be fit to time series models? Again, sensitivty to unforeseen externalities is paramount.\n",
    "\n",
    "As it stands, the model and the Streamlit app form a foundation on which to build in exploring further."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
